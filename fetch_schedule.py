# -*- coding: utf-8 -*-
import os
import json
import time
import httpx
import datetime
import re
import html
from bs4 import BeautifulSoup
from opencc import OpenCC

# é…ç½®
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
SCHEDULE_FILE = os.path.join(BASE_DIR, "static", "json", "schedule.json")
COVER_MAP_FILE = os.path.join(BASE_DIR, "static", "json", "cover_map.json")

HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://anime1.me/"
}

client = httpx.Client(headers=HEADERS, timeout=30.0, follow_redirects=True)
cc = OpenCC('t2s')

# åŠ è½½å°é¢æ˜ å°„
COVER_MAP = {}
if os.path.exists(COVER_MAP_FILE):
    try:
        with open(COVER_MAP_FILE, 'r', encoding='utf-8') as f:
            COVER_MAP = json.load(f)
    except:
        pass

# è·å– safe_id_set
SAFE_ID_SET = set()
def fetch_safe_ids():
    print("[INFO] æ­£åœ¨è·å–å…¨ç«™ç•ªå‰§ç™½åå•...")
    try:
        timestamp = int(time.time() * 1000)
        res = client.get(f"https://anime1.me/animelist.json?_={timestamp}")
        data = res.json()
        count = 0
        for item in data:
            raw_id = item[0]
            if isinstance(raw_id, int) and raw_id > 0:
                SAFE_ID_SET.add(str(raw_id))
                count += 1
            else:
                raw_title = item[1]
                match = re.search(r'cat=(\d+)', raw_title)
                if match:
                    SAFE_ID_SET.add(match.group(1))
                    count += 1
        print(f"[INFO] ç™½åå•è·å–å®Œæ¯•: {count} æ¡")
    except Exception as e:
        print(f"[ERROR] ç™½åå•è·å–å¤±è´¥: {e}")

def get_current_season_score():
    now = datetime.datetime.now()
    month = now.month
    if 1 <= month <= 3: season = "å†¬å­£"
    elif 4 <= month <= 6: season = "æ˜¥å­£"
    elif 7 <= month <= 9: season = "å¤å­£"
    else: season = "ç§‹å­£"
    s_map = {"å†¬å­£": 1, "æ˜¥å­£": 2, "å¤å­£": 3, "ç§‹å­£": 4}
    return now.year * 10 + s_map[season], now.year, season

def get_season_score(year, season):
    s_map = {"å†¬å­£": 1, "æ˜¥å­£": 2, "å¤å­£": 3, "ç§‹å­£": 4}
    return int(year) * 10 + s_map.get(season, 0)

def fetch_single_season(year, season):
    url = f"https://anime1.me/{year}å¹´{season}æ–°ç•ª"
    print(f"ğŸ”„ æ­£åœ¨çˆ¬å–: {year} {season} ({url})...")
    
    try:
        res = client.get(url)
        if res.status_code != 200:
            return None
        
        soup = BeautifulSoup(res.text, 'html.parser')
        table = soup.find('table')
        if not table: return None
            
        week_data = [[] for _ in range(7)]
        tbody = table.find('tbody')
        if not tbody: return None
        
        rows = tbody.find_all('tr')
        for row in rows:
            cols = row.find_all('td')
            for col_idx, col in enumerate(cols):
                if col_idx >= 7: break
                
                link = col.find('a')
                if link:
                    title_raw = link.text.strip()
                    href = link.get('href', '')
                    
                    cat_id = None
                    m = re.search(r'cat=(\d+)', href)
                    if m: cat_id = m.group(1)
                    
                    if cat_id and title_raw:
                        if SAFE_ID_SET and cat_id not in SAFE_ID_SET:
                            continue
                            
                        clean_tc = html.unescape(title_raw)
                        title_sc = cc.convert(clean_tc)
                        
                        # è¿™é‡Œä¾ç„¶å°è¯•è·å–ä¸€ä¸‹ï¼Œä½†ä¸»è¦ä¾é æœ€åçš„ç»Ÿä¸€åˆ·æ–°
                        poster = ""
                        if title_sc in COVER_MAP and COVER_MAP[title_sc]:
                             poster = f"/covers/{COVER_MAP[title_sc]}"
                             
                        week_data[col_idx].append({
                            "id": cat_id,
                            "title": title_sc,
                            "poster": poster,
                            "year": str(year),
                            "season": season
                        })
        print(f"   âœ… è·å–æˆåŠŸ")
        return week_data
    except Exception as e:
        print(f"   âŒ å‡ºé”™: {e}")
        return None

def main():
    fetch_safe_ids()
    
    schedule_cache = {}
    if os.path.exists(SCHEDULE_FILE):
        try:
            with open(SCHEDULE_FILE, 'r', encoding='utf-8') as f:
                schedule_cache = json.load(f)
            print(f"[INFO] å·²åŠ è½½æœ¬åœ°ç¼“å­˜: {len(schedule_cache)} ä¸ªå­£åº¦")
        except:
            pass
            
    current_score, curr_year, curr_season = get_current_season_score()
    
    start_year = 2017
    seasons = ["å†¬å­£", "æ˜¥å­£", "å¤å­£", "ç§‹å­£"]
    
    targets = []
    for y in range(start_year, curr_year + 1):
        for s in seasons:
            score = get_season_score(y, s)
            if score < 20172: continue
            if score > current_score: continue
            targets.append((y, s))
            
    has_update = False
    
    # --- çˆ¬å–é€»è¾‘ ---
    for year, season in targets:
        score = get_season_score(year, season)
        key = f"{year}_{season}"
        
        needs_fetch = False
        is_current = (score == current_score)
        
        if is_current:
            print(f"[CHECK] {key} (å½“å‰å­£åº¦) -> å¼ºåˆ¶æ›´æ–°")
            needs_fetch = True
        else:
            if key not in schedule_cache or not schedule_cache[key]:
                print(f"[CHECK] {key} (ç¼ºå¤±) -> éœ€è¡¥å…¨")
                needs_fetch = True
            
        if needs_fetch:
            data = fetch_single_season(year, season)
            if data:
                schedule_cache[key] = data
                has_update = True
                time.sleep(1.0)

    # --- æ–°å¢é€»è¾‘ï¼šå³ä½¿ä¸çˆ¬å–ï¼Œä¹Ÿè¦åˆ·æ–°æ‰€æœ‰æœ¬åœ°ç¼“å­˜çš„å°é¢ ---
    print("[INFO] æ­£åœ¨æ ¡éªŒå¹¶åˆ·æ–°æ‰€æœ‰å­£åº¦å°é¢...")
    updated_covers_count = 0
    
    # éå†ç¼“å­˜ä¸­æ‰€æœ‰çš„å­£åº¦
    for key in schedule_cache:
        week_data = schedule_cache[key]
        # éå†æ¯ä¸€å¤©
        for day_list in week_data:
            # éå†æ¯ä¸€éƒ¨ç•ª
            for anime in day_list:
                title = anime.get('title')
                # æ£€æŸ¥ cover_map é‡Œæ˜¯å¦æœ‰è¿™ä¸ªç•ªçš„å°é¢
                if title in COVER_MAP and COVER_MAP[title]:
                    new_poster_path = f"/covers/{COVER_MAP[title]}"
                    
                    # å¦‚æœå½“å‰ JSON é‡Œçš„å°é¢åœ°å€å’Œ Map é‡Œçš„ä¸ä¸€æ ·ï¼ˆæˆ–è€…æ˜¯ç©ºçš„ï¼‰
                    # å°±æ›´æ–°å®ƒï¼Œå¹¶æ ‡è®° has_update = True ä»¥ä¾¿ä¿å­˜
                    if anime.get('poster') != new_poster_path:
                        anime['poster'] = new_poster_path
                        updated_covers_count += 1
                        has_update = True

    if updated_covers_count > 0:
        print(f"[INFO] â™»ï¸  å·²æ›´æ–° {updated_covers_count} ä¸ªç•ªå‰§çš„å°é¢é“¾æ¥")

    # 5. ä¿å­˜
    if has_update:
        try:
            with open(SCHEDULE_FILE, 'w', encoding='utf-8') as f:
                json.dump(schedule_cache, f, ensure_ascii=False)
            print("[SUCCESS] æ‰€æœ‰æ›´æ–°ï¼ˆå«å°é¢ï¼‰å·²ä¿å­˜åˆ° schedule.json")
        except Exception as e:
            print(f"[ERROR] ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
    else:
        print("[INFO] æ•°æ®æ— å˜åŒ–ï¼Œæ— éœ€ä¿å­˜")

if __name__ == "__main__":
    main()